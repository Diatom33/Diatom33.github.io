<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Merged RSS Feed</title>
    <description>Combined feed from multiple sources</description>
    <lastBuildDate>Sun, 23 Nov 2025 06:25:08 GMT</lastBuildDate>
    <generator>GitHub Actions RSS Merger</generator>
    
    <item>
      <title><![CDATA[Gemini 3 is Evaluation-Paranoid and Contaminated]]></title>
      <link>https://www.lesswrong.com/posts/8uKQyjrAgCcWpfmcs/gemini-3-is-evaluation-paranoid-and-contaminated</link>
      <description><![CDATA[<p> <i>TL;DR: Gem­ini 3 fre­quently thinks it is in an eval­u­a­tion when it is not, as­sum­ing that all of its re­al­ity is fabri­cated. It can also re­li­ably out­put the </i><a href="https://github.com/google/BIG-bench"><i>BIG-bench</i></a><i> ca­nary string, in­di­cat­ing that Google likely t......]]></description>
      <pubDate>Thu, 20 Nov 2025 21:02:24 GMT</pubDate>
      <guid>8uKQyjrAgCcWpfmcs</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[MLSN #17: Measuring General AI Abilities and Mitigating Deception]]></title>
      <link>https://www.lesswrong.com/posts/kmu8y5beCEHHz783k/mlsn-17-measuring-general-ai-abilities-and-mitigating</link>
      <description><![CDATA[<p>TLDR: New met­rics say that fron­tier AIs get 57% on tests for gen­eral in­tel­li­gence and are able to do 2.5% of re­mote free­lance-type work.</p><p>Many bench­marks mea­sure AIs on use­ful knowl­edge and abil­ities, but there is a mea­sure­ment gap around some of the most im­por­tant ca­pa­......]]></description>
      <pubDate>Wed, 19 Nov 2025 20:11:53 GMT</pubDate>
      <guid>kmu8y5beCEHHz783k</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[MLSN #17: Measuring General AI Abilities and Mitigating Deception]]></title>
      <link>https://newsletter.mlsafety.org/p/mlsn-17-measuring-general-ai-abilities</link>
      <description><![CDATA[Measuring General AI Abilities...]]></description>
      <pubDate>Wed, 19 Nov 2025 20:00:57 GMT</pubDate>
      <guid>https://newsletter.mlsafety.org/p/mlsn-17-measuring-general-ai-abilities</guid>
      <category>Newsletter</category>
      <source>Newsletter</source>
    </item>
    <item>
      <title><![CDATA[In-Context Writing with Sonnet 4.5]]></title>
      <link>https://www.lesswrong.com/posts/LmxaJQya85ESDo9LF/in-context-writing-with-sonnet-4-5</link>
      <description><![CDATA[<p><i>All of the writ­ing in this post is my own, with­out any LLM in­put.</i></p><p>We’ve already seen a bit of how Claude Son­net 4.5 does with <a href="/posts/SwiChH68fRERBiCHe/experiments-with-sonnet-4-5-s-fiction">writ­ing fic­tion</a> (bet­ter than pre­vi­ous mod­els, but not quite good yet......]]></description>
      <pubDate>Mon, 17 Nov 2025 07:51:38 GMT</pubDate>
      <guid>LmxaJQya85ESDo9LF</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[AISN #65: Measuring Automation and Superintelligence Moratorium Letter]]></title>
      <link>https://www.lesswrong.com/posts/wD8tdEbtMSrcSfJmM/aisn-65-measuring-automation-and-superintelligence</link>
      <description><![CDATA[<p>Wel­come to the AI Safety Newslet­ter by the <a href="https://www.safe.ai/">Cen­ter for AI Safety</a>. We dis­cuss de­vel­op­ments in AI and AI safety. No tech­ni­cal back­ground re­quired.</p><p>In this edi­tion: A new bench­mark mea­sures AI au­toma­tion; 50,000 peo­ple, in­clud­ing top AI s......]]></description>
      <pubDate>Wed, 29 Oct 2025 16:05:26 GMT</pubDate>
      <guid>wD8tdEbtMSrcSfJmM</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[Uncommon Utilitarianism #3: Bounded Utility Functions]]></title>
      <link>https://www.lesswrong.com/posts/jvXF88XmqtsR7uE4w/uncommon-utilitarianism-3-bounded-utility-functions</link>
      <description><![CDATA[<p><a href="/posts/FGEHXmK4EnXK6A6tA/uncommon-utilitarianism-2-positive-utilitarianism">Pre­vi­ous Post</a></p><p>For con­text on how I dis­cuss util­i­tar­i­anism in this se­quence, read the <a href="/posts/NRxn6R2tesRzzTBKG/sublinear-utility-in-population-and-other-uncommon">first post</a>.</p>......]]></description>
      <pubDate>Mon, 27 Oct 2025 05:06:25 GMT</pubDate>
      <guid>jvXF88XmqtsR7uE4w</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[Uncommon Utilitarianism #2: Positive Utilitarianism]]></title>
      <link>https://www.lesswrong.com/posts/FGEHXmK4EnXK6A6tA/uncommon-utilitarianism-2-positive-utilitarianism</link>
      <description><![CDATA[<p><a href="/posts/NRxn6R2tesRzzTBKG/sublinear-utility-in-population-and-other-uncommon"><i>Pre­vi­ous Post</i></a>—for con­text on how I dis­cuss util­i­tar­i­anism for this se­quence, read this.</p><h1>The Literal Meaning</h1><p>Many peo­ple have heard of <a href="https://en.wikipedia.org/wiki/......]]></description>
      <pubDate>Mon, 20 Oct 2025 04:17:45 GMT</pubDate>
      <guid>FGEHXmK4EnXK6A6tA</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[Sublinear Utility in Population and other Uncommon Utilitarianism]]></title>
      <link>https://www.lesswrong.com/posts/NRxn6R2tesRzzTBKG/sublinear-utility-in-population-and-other-uncommon</link>
      <description><![CDATA[<p><i>Con­tent warn­ing: An­throp­ics, Mo­ral Philos­o­phy, and Shrimp</i></p><p><i>This post isn’t try­ing to be self con­tained, since I have so many dis­parate thoughts about this. In­stead, I’m try­ing to put a rep­re­sen­ta­tive set of ideas for­ward, and I hope that if peo­ple are in­ter­es......]]></description>
      <pubDate>Mon, 13 Oct 2025 06:19:52 GMT</pubDate>
      <guid>NRxn6R2tesRzzTBKG</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[Alignment Faking Demo for Congressional Staffers]]></title>
      <link>https://www.lesswrong.com/posts/55CBrLrXiQgHBb2gF/alignment-faking-demo-for-congressional-staffers</link>
      <description><![CDATA[<p>In Fe­bru­ary 2025, <a href="https://awestover.github.io/">Alek Westover</a> and I pre­sented a ~5-minute talk and demo about <a href="https://www.anthropic.com/research/alignment-faking">Align­ment Fak­ing</a> to sev­eral groups of con­gres­sional staffers in­ter­ested in AI at an event hoste......]]></description>
      <pubDate>Mon, 06 Oct 2025 01:44:03 GMT</pubDate>
      <guid>55CBrLrXiQgHBb2gF</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
    <item>
      <title><![CDATA[Applied Murphyjitsu Meditation]]></title>
      <link>https://www.lesswrong.com/posts/YTfXJzrzWGj8mjQ2t/applied-murphyjitsu-meditation</link>
      <description><![CDATA[<p><i>Ex­panded and gen­er­al­ized ver­sion of </i><a href="/posts/eukutrwGBPutoQoyM/alice-blair-s-shortform#comment-riwHkC6KnMpYpLFro"><i>this shortform</i></a></p><h1>Mo­ti­va­tion: Typ­ing Fast</h1><p>Part of my writ­ing pro­cess in­volves get­ting words out of my head and into a text ed­i­tor......]]></description>
      <pubDate>Mon, 29 Sep 2025 06:31:44 GMT</pubDate>
      <guid>YTfXJzrzWGj8mjQ2t</guid>
      <category>LessWrong</category>
      <source>LessWrong</source>
    </item>
  </channel>
</rss>