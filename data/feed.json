{
  "meta": {
    "title": "Merged RSS Feed",
    "description": "Combined feed from multiple sources",
    "lastUpdated": "2025-08-31T12:31:01.143Z",
    "totalItems": 10,
    "sources": [
      {
        "title": "Alice Blair's Posts - LessWrong 2.0 viewer",
        "description": "Alice Blair's Posts - LessWrong 2.0 viewer",
        "link": "https://www.greaterwrong.com/",
        "category": "LessWrong",
        "name": "LessWrong",
        "dateFilter": null
      },
      {
        "title": "ML Safety Newsletter",
        "description": "ML Safety Research News",
        "link": "https://newsletter.mlsafety.org",
        "category": "Newsletter",
        "name": "Newsletter",
        "dateFilter": "2025-05-01T00:00:00.000Z"
      }
    ],
    "generatedBy": "GitHub Actions RSS Merger"
  },
  "items": [
    {
      "title": "ML Safety Newsletter #15",
      "link": "https://newsletter.mlsafety.org/p/ml-safety-newsletter-15",
      "description": "Risks in Agentic Computer Use, Goal Drift, Shutdown Resistance, and Critiques of Scheming Research",
      "pubDate": "Mon, 18 Aug 2025 17:08:04 GMT",
      "isoDate": "2025-08-18T17:08:04.000Z",
      "author": "Alice Blair",
      "guid": "https://newsletter.mlsafety.org/p/ml-safety-newsletter-15",
      "category": "Newsletter",
      "source": "Newsletter",
      "feedTitle": "ML Safety Newsletter"
    },
    {
      "title": "Handing People Puzzles",
      "link": "https://www.lesswrong.com/posts/xRZfpYkC9MwqG4tKi/handing-people-puzzles",
      "description": "We’re a com­mu­nity that is es­pe­cially vuln­er­a­ble to nerd sniping, as com­mu­ni­ties go. I’m fond of par­tak­ing in a lit­tle nerd sniping my­self. While the origi­nal xkcd paints this pas­time in a mal­i­cious light, I ar­gue that this is in fact a great thing in gen­eral. One of my fa­vorite ways to do this, and one that I think the ra­tio­nal­ist com­mu­nity is ne­glect­ing, is Hand­ing Peo­ple Puz­zles. \nAt Meetups\nI once spent a few weeks go­ing to var­i­ous ra­tio­nal­ist mee­tups in Berkeley, and al­most ev­ery per­son I’d meet, I’d start the con­ver­sa­tion by silently hand­ing them a small 3D printed puz­zle with two pieces that fit to­gether in an un­in­tu­itive way. They go for a hand shake? I reach out with a puz­zle in my hand. They go for small talk or “So what do you do?”, I smile and hand them a puz­zle. I know I’m not the only per­son to ever do this, and hope­fully this post means I’m far from the last. \nThe re­views for this tech­nique were all around highly en­thu­si­as­tic. Things that I think worked re­ally well:\n\nIt broke the ex­ist­ing con­ver­sa­tional mold re­ally well: Peo­ple were no longer stuck in vari­a­tions of “So what do you do?”. Really com­mit­ting to the act, turn­ing even nor­mal greet­ings like hand­shakes into some­thing else re­ally drives this point home.\n\nIt was a gen­uinely en­gag­ing puz­zle: one out­lier solved it in about 30 sec­onds, the rest of the peo­ple took 15-30 min­utes. You can vary puz­zle difficulty based on how long you want this win­dow to be. Most peo­ple can talk while solv­ing this type of fit-things-to­gether puz­zle since it’s en­tirely non­ver­bal, but their at­ten­tion is still di­vided.\n\nIt’s mem­o­rable: peo­ple knew that I was the one hand­ing out the puz­zles, and I sus­pect that that gave them a han­dle by which to re­mem­ber other things about me bet­ter. \n\nThere are some con­di­tional draw­backs:\n\nSome peo­ple re­ally do just want to talk at the mee­tups, and their puz­zling life is already sat­u­rated. Let the con­ver­sa­tion flow a differ­ent way in­stead.\n\nYou may want to have shorter con­ver­sa­tions than the solve time for any suffi­ciently in­ter­est­ing puz­zle, or you may want to have the per­son’s full and un­di­vided at­ten­tion, even the parts that would go to back­ground puz­zle fid­get­ing. This strat­egy is pretty in­com­pat­i­ble with that stance.\n\nAt Houses\nRa­tion­al­ists tend to have rather dis­tinc­tive in­te­rior de­sign, and this is no ac­ci­dent. There are the stan­dard hal­l­marks of a nice space: comfy couches, warm light­ing, and blan­kets. There are also the hal­l­marks of a gath­er­ing space for of­ten deeply nerdly peo­ple: piles of books, gi­ant white­boards, and some­times, though not of­ten enough, puz­zles strewn about.\nWhen some­body comes into a space for the first time, they grasp for some way to en­gage with it: they sit down on the couch, find some­thing to di­rect their at­ten­tion to. Hope­fully, they find some­thing puz­zling on the table to play with, some­thing that piques their cu­ri­os­ity and makes them strug­gle with some­thing novel for a lit­tle bit, for this is the ra­tio­nal­ist way. \nI re­ally en­joy when some­one new comes into my house and picks up some­thing that fas­ci­nates them, and I think this is gen­er­ally good for the same rea­sons that I listed about meetup-type so­cial in­ter­ac­tions.\nPuz­zle Recom­men­da­tions (Not Ex­haus­tive!)\n\nThe puz­zle I used in my anec­dote is the fifth of these from Ge­orge Hart.\n\nRu­bik’s Cubes are clas­sic, but usu­ally ei­ther too easy (for peo­ple who know how) or too hard (for peo­ple who don’t). Var­i­ants that I think are great for this pur­pose are:\n\nDino Cube (peo­ple can usu­ally figure this out with­out pre­vi­ous knowledge\n\nThe 2x2 Ru­bik’s Cube, which is sig­nifi­cantly eas­ier than the 3x3 but still non­triv­ial for peo­ple who aren’t fa­mil­iar.\n\n\n\nMy house has a Brain­string R (other similar puz­zles available at that link) which looks re­ally cool, but is bro­ken so I haven’t tried it. Even still, it suc­cess­fully grabbed the at­ten­tion of at least one vis­i­tor and sparked a con­ver­sa­tion.\n\nAno­ma­lous ob­jects that aren’t clas­si­cally “puz­zles” but that can be very puz­zling:\n\nThe MetMo Pis­ton, a pre­ci­sion ma­chined air pis­ton that be­haves pretty cu­ri­ously.\n\nMu­si­cal in­stru­ments that are un­com­mon in Western mu­sic and easy to make a sound on:\n\nOcari­nas (you can also 3D print these!)\n\nJaw harps (you can also use a hair tie as a low-qual­ity one, but that doesn’t quite fit the pur­pose of this post.)\n\n\n\nJeweller’s Loupes, cheap hand­held micro­scopes with a sur­pris­ing amount of mag­nifi­ca­tion. Depend­ing on light­ing, you can see some peo­ple’s in­di­vi­d­ual skin cells. I can­not over­state how fun it is to point it at var­i­ous things in your en­vi­ron­ment. This ex­er­cise also leads to puz­zles like “Why does the couch weave look like that?” or “Why do skin cells look like that?”",
      "pubDate": "Mon, 18 Aug 2025 06:27:58 +0000",
      "isoDate": "2025-08-18T06:27:58.000Z",
      "author": "Alice Blair",
      "guid": "xRZfpYkC9MwqG4tKi",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "Listening Before Speaking",
      "link": "https://www.lesswrong.com/posts/HT7wTWNdtqmiJ4veE/listening-before-speaking",
      "description": "Epistemic Sta­tus: anec­data and intuition\ned­ited GPTl;dr: For so­cially trans­mit­table skills that re­quire learn­ing lots of new cat­e­gory bound­aries (lan­guages, sub­cul­tures, etc.), a de­liber­ate in­put-heavy out­put-light phase at the be­gin­ning re­duces fos­silized er­rors and speeds later fluency.\nLan­guage Learning\nA friend of mine, let’s call him Bob, learned English out­side of his crit­i­cal lan­guage ac­qui­si­tion pe­riod, the time early in one’s life when fluently learn­ing a lan­guage is prac­ti­cally guaran­teed, rel­a­tive to the difficul­ties peo­ple face later in life. Usu­ally this would im­ply that Bob has some sort of for­eign-sound­ing ac­cent, pos­si­bly re­tain­ing some of the gram­mar and syn­tax of his na­tive lan­guage in­stead of that of English.\nYet Bob speaks fluent, na­tive-sound­ing Gen­eral Amer­i­can English. He knows about as many words as the na­tive speak­ers around him, with some small gaps. I ar­gue that he’s done this by mir­ror­ing not only the pat­tern that ba­bies use to learn lan­guage, but also by mir­ror­ing a more gen­eral type of strat­egy for fluently learn­ing rad­i­cally new types of com­mu­ni­ca­tion: Listen­ing be­fore Speak­ing.\nBob spent around 6 years con­sum­ing me­dia in English be­fore re­ally ever speak­ing. I don’t think this pro­cess needed to take this long, but this sort of scale seems ap­prox­i­mately right. After get­ting a bit of run­way to work with, he would watch English YouTube and TV shows with­out any sub­ti­tles, see­ing how much he could un­der­stand just through what he already knew. I ar­gue this did sev­eral things that were helpful:\n\nGive him time to un­der­stand the phonemes in English with­out im­me­di­ately im­pos­ing his na­tive set of phonemes onto them. When a na­tive English speaker, call her Carol, first tries to learn Span­ish, she might parse the rol­led r as “sounds like the English r with some for­eign ac­cent”, and then go to try and say “bur­rito” with the English r, hav­ing in­cor­rectly as­sumed it’s the same thing that she was already fa­mil­iar with from English. Spend­ing more time Listen­ing al­lows a dis­tinct con­cept bound­ary to form around the ex­act de­tails of the Span­ish rol­led r: where in the mouth ex­actly is it? How many os­cilla­tions tend to oc­cur in it? etc. Bob can now draw that con­cept bound­ary much more pre­cisely than Carol can, and much more similarly to how a na­tive speaker would, even if he’s work­ing en­tirely on in­tu­ition and has no ex­plicit, crys­tal­lized knowl­edge about any of the lin­guis­tics in­volved.\n\nForce him to ac­tu­ally rely on his map of English, rather than his map of any other lan­guage. Without sub­ti­tles or trans­la­tions, there’s much less “ohhh, that’s the word for [na­tive word]” and much more “huh, why did the [English word] have to do [English word]?” Work­ing en­tirely in the English frame of refer­ence al­lows his model English to fail fast and up­date quickly. It also trains the mus­cle that he’ll need later, where he ex­clu­sively thinks in the English frame of refer­ence be­cause it’s manda­tory to, such as real-time in­ter­ac­tions.\n\nHe’s now up to 3 lan­guages fluently and is learn­ing an­other cur­rently.\n\nCul­ture Learning\nSome­times, some­one new shows up in the in-per­son ra­tio­nal­ist scene. Call him Dan. Three types of thing can hap­pen then:\n\nHe doesn’t say much, and just sits and ab­sorbs the conversation\n\nHe en­gages ac­tively in the conversation\n\nand what he’s say­ing makes sense and sounds nor­mal to the rest of the people\n\nand he is ob­vi­ously fum­bling around and e.g. mak­ing weird in­fer­ences that The Se­quences cau­tion against. Many find these sorts to be an­noy­ing, al­though many are wel­com­ing to the new­comer.\n\n\n\nTh­ese cor­re­spond to\n\nListen­ing be­fore Speak­ing (Listen­ing Phase)\n\nListen­ing be­fore Speak­ing (Speak­ing Phase)\n\nSpeak­ing be­fore Listening\n\nIn the cases where Dan hasn’t yet Listened, he also hasn’t ac­cul­turated fully to the ra­tio­nal­ist scene, but in one case this comes across as an is­sue in a well-kept gar­den, and in the other he’s rel­a­tively harm­less. \nDan fits in when he Listens be­fore Speak­ing, which usu­ally hap­pens by read­ing what he can from the ra­tio­nal­ist ma­te­rial on­line. He learns the dis­tinc­tions and con­cepts well ahead of ac­tu­ally try­ing to use them, and doesn’t get tripped up by Dun­ning-Kruger. He has already ab­sorbed some pow­ers from the com­mu­nity for him­self, and oth­ers can see that he is already One Of Them. \nDans who fum­ble like this may be highly in­tel­li­gent and well-read out­side of ra­tio­nal­ity, but they can re­li­ably be clas­sified as not hav­ing Listened be­fore Speak­ing. This is the sort of com­mu­nity we have, whether we like it or not, where learn­ing our ways is in this sense like learn­ing a new lan­guage.\nConclusion\nIn all of these cases, it took a lot of ac­tual effort to get to a state of fluency. Bob’s learn­ing didn’t come from pas­sively ab­sorb­ing me­dia so much as hun­grily dis­sect­ing it. Similarly, peo­ple of­ten seek out ra­tio­nal­ity for their own ends, since af­ter all there is some­thing real worth learn­ing ra­tio­nal­ity for. This is not the only way to learn things fluently: some peo­ple are pho­net­i­ci­ans and can pro­nounce a new lan­guage cor­rectly on the first try, some peo­ple have more of the Art of ra­tio­nal­ity within them by de­fault. Still, this is a use­ful tac­tic, all else be­ing equal.",
      "pubDate": "Mon, 11 Aug 2025 05:23:00 +0000",
      "isoDate": "2025-08-11T05:23:00.000Z",
      "author": "Alice Blair",
      "guid": "HT7wTWNdtqmiJ4veE",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "Notes from Dopamine Detoxing",
      "link": "https://www.lesswrong.com/posts/BeXGMt7gmGFXFerMC/notes-from-dopamine-detoxing",
      "description": "[Epistemic sta­tus: I’m not a neu­ro­scien­tist nor any kind of med­i­cal nor biolog­i­cal pro­fes­sional. I have an am­a­teur un­der­stand­ing of neu­ro­chem­istry and cog­ni­tive sci­ence. This post is pri­mar­ily based on what has worked for me and my in­tu­itions around why. I err on the side of epistemic trans­parency and cau­tion where pos­si­ble.]\nPr­ereq­ui­site Con­cept: He­donic Set Point\nDis­claimers/​Warnings\nI think this can be a gen­uinely very use­ful tech­nique for peo­ple. How­ever,\nThis can Prob­a­bly be Dangerous\nThis is a post about do­ing some light amount of hack­ing your brain’s chem­i­cals, speci­fi­cally in a way that in­volves tem­porar­ily tak­ing away en­joy­able things. Do not do this if you think that would be long-term bad for your men­tal health. This is not a tech­nique for deal­ing with e.g. se­vere de­pres­sion (nor prob­a­bly most smaller-mag­ni­tude de­pres­sions). This is not a tech­nique for fix­ing com­plex men­tal health is­sues, it is a tech­nique for deal­ing with a very spe­cific thing in some sub­set of peo­ple whose brains are some­what like mine along some rele­vant axes.\nI think there are some men­tal states and neu­rotypes where this is an ac­tively harm­ful tech­nique if you try to push through it for >0.5 days when that isn’t the right move. I sus­pect it’s rel­a­tively safe if you have the mind­ful­ness to re­al­ize if things are mak­ing you long-term worse off and im­me­di­ately Nope Out of the detox (de­scribed near the end). This is worth be­ing cau­tious about.\nAnd I’m No­tice­ably Biased\nAlso, this post is about a men­tal tech­nique that \n\nmade me feel good af­ter do­ing it\n\nre­cently tem­porar­ily fixed what hap­pened to be the most [big × tractable] bot­tle­neck to my pro­duc­tivity. \n\nFor these rea­sons, the pic­ture I paint of the benefits may be rosy and por­tray an effect size that does not gen­er­al­ize re­li­ably to other peo­ple. Given those forces act­ing upon me, I try to paint a true pic­ture and con­vey a gen­eral tech­nique to what­ever ex­tent I rea­son­ably can.\nIntroduction\n(You can prob­a­bly skip this sec­tion if you already know what a dopamine detox is and why you might want to do one.)\nA “dopamine detox” is, for the pur­poses of this post, a pe­riod (usu­ally a day) in which you try re­ally hard to get rid of su­per­stim­uli in your en­vi­ron­ment. I’ve writ­ten in some prob­a­bly-helpful ad­di­tional de­tail about how su­per­stim­uli can hack your cog­ni­tion, but this post does most of the nec­es­sary con­cep­tual ex­plain­ing.\nI’d ap­pre­ci­ate if any­one who tries this tech­nique be­cause of this post re­views it, since my anec­data is limited to my own ex­pe­rience and that of a small num­ber of oth­ers.\nSu­per­stim­uli and Detoxing\nStim­u­lat­ing ac­tivi­ties give peo­ple a lit­tle bit of dopamine and make them more likely to do that ac­tivity in the fu­ture. This is mostly fine within the ap­prox­i­mate an­ces­tral dis­tri­bu­tion of stim­uli: you eat a tasty, healthy salad, you like the salad[1], you con­tinue eat­ing sal­ads when you’re hun­gry for them, and this gets you util­ity. Im­por­tantly, the amount of stim­u­lus you get from eat­ing a salad causes you to eat more sal­ads ap­prox­i­mately the op­ti­mal amount, from a util­ity-max­i­miz­ing point of view. You don’t have to do con­scious op­ti­miza­tion over how of­ten you eat sal­ads, the re­ward-seek­ing part of your brain does that au­to­mat­i­cally. A su­per­stim­u­lus, for the pur­poses of this post, is some­thing where this prop­erty is not true.\nPizza is a su­per­stim­u­lus[1]. Pizza is en­tirely out­side even the far reaches of the an­ces­tral tasty food dis­tri­bu­tion available to hu­mans. It is not, how­ever, cor­re­spond­ingly healthy. By de­fault, many hu­man brains ex­posed to pizza get hacked by the den­sity of the stim­u­lus, and pur­sue pizza more than is util­ity-max­i­miz­ing (even af­ter ac­count­ing for the fact that ex­pe­rienc­ing tasty food is also util­ity).\nAd­di­tion­ally, in the con­text of abun­dant pizza, healthy sal­ads seem much less ap­peal­ing. Pizza moves the he­do­nic set point up­wards, and sud­denly sal­ads don’t seem so ap­pe­tiz­ing in com­par­i­son. Beyond just re­ceiv­ing un­due pri­or­ity among foods, pizza com­petes against other types of stim­uli as well: a higher he­do­nic set point makes “write a long LessWrong post about med­i­ta­tion” seem less fun than it would be oth­er­wise (i.e. usu­ally pretty fun).\nA dopamine detox is about re­set­ting the he­do­nic set point to “nor­mal” so that sub­jec­tive re­ward cor­re­sponds more closely to util­ity.[2]\nThe next sec­tion fol­lows an archetyp­i­cal pro­gres­sion of a dopamine detox. I have had sev­eral ex­pe­riences sep­a­rated by months which closely match this pro­gres­sion, but this de­scrip­tion is not a liter­ally true story of any one par­tic­u­lar dopamine detox.\nThe De­tox Experience\nThe Day Before\nThe day be­fore the detox is es­pe­cially filled with the stim­u­lus-noise that fights for my at­ten­tion ev­ery day. My friends send me memes. I scroll Sub­stack. I eat pizza. I get a cou­ple hours of work done, but I have a cou­ple times where I sit down at my com­puter, find a video of some­one do­ing some­thing en­ter­tain­ing, and then the next thing I know it’s mys­te­ri­ously two hours later. \nI say oops. I no­tice that some­thing hap­pened that I wish hadn’t. I open up my jour­nal and start dump­ing out my thoughts, but my mind feels slug­gish. It’s not im­me­di­ately clear what’s go­ing on, just that some­thing weird is go­ing on in my brain. My mind wan­ders back to that video I was watch­ing ear­lier. I al­most open up Youtube again, but I stop my­self. Oops again. \nI’ve been through this enough times to no­tice what’s hap­pen­ing: I’ve been af­flicted by the su­per­stim­uli. I delete Youtube from my phone and ad­just my browser’s site blocker. It’s get­ting late though, and I should sleep in­stead of try­ing to op­ti­mize any harder. To­mor­row, though, I’ll do a dopamine detox.\nThe Day Of\nI wake up to my phone’s alarm. I con­sciously choose to not check my no­tifi­ca­tions, and in­stead only look at my phone long enough to turn on Do Not Dis­turb. I have filters de­signed to let through emer­gency com­mu­ni­ca­tions, but I pre­com­mit to check my mes­sages at mid­day for any ur­gent-pri­or­ity in­ter­rrups any­ways. And so I go about my day.\nIt hurts, not like in­jury or sad­ness, but like ex­er­cise and difficult work. It hurts like Ac­tu­ally Try­ing hurts, some­times. There is a part of my brain that is nag­ging in the back­ground for su­per­stim­uli. I sit down to eat, and it nags for me to read some­thing on my phone. I sit down to work, and it nags me to check Dis­cord. The theme of the day, how­ever, is that I ex­pend willpower to do Not That. \nIt’s tax­ing, both on my willpower to avoid su­per­stim­uli and on my mind­ful­ness to no­tice when that’s hap­pen­ing. But left with noth­ing more stim­u­lat­ing to do, I write: I jour­nal, I do work for my job (which also hap­pens to be writ­ing), and I write a LessWrong post. There are sev­eral mo­ments when I don’t have the en­ergy to do any­thing detox-al­lowed but sit and ex­ist, but I choose those in­stead of the su­per­stim­uli.\nI go to bed. My brain is still nois­ily nag­ging me about things as I try to sleep, but I am Tired and the Tired­ness wins.\nThe Days After\nI feel no­tice­ably less bad than yes­ter­day! Sleep­ing is good for be­com­ing less tired, as well as for ad­just­ing the he­do­nic set point. I go out­side. Sun­sh­ine and birds and trees are un­usu­ally pleas­ant. I talk with a friend. Talk­ing with peo­ple is re­ally pleas­ant! \nI very care­fully rein­tro­duce some limited su­per­stim­uli, since I no longer want to spend willpower on them. I read a fun ar­ti­cle, I check Dis­cord, I eat some candy when it’s offered. They are un­usu­ally pleas­ant, but that plea­sure is not the main point of a dopamine detox.\nThe real benefits are in the nor­mal-level stim­uli. Work­ing on some­thing more difficult than eat­ing candy is sud­denly very doable. My work­flow be­comes closer to be­ing the de­fault, in­stead of some­thing that takes non-neg­ligible willpower. I’m more read­ily able to form habits and en­ter deep work.\nI find that I get less lost when hav­ing com­plex con­ver­sa­tions, and I or­ga­nize my thoughts bet­ter when ex­plain­ing my­self. I pick up on sub­tle things in my en­vi­ron­ment that I didn’t no­tice be­fore in all of the stim­u­lat­ing noise.\nIm­por­tantly, dopamine detox­ing puts me in a headspace where pre­serv­ing these benefits into the fu­ture is less tax­ing. Some­times I fail and need to detox again, but the de­fault ac­tion in most cases is to keep be­ing not-hacked by stim­uli and keep be­ing un­usu­ally pro­duc­tive. The “wow birds and trees and sun­sh­ine are way cooler than I re­mem­ber” effect is mostly a tem­po­rary ar­ti­fact of the rapid change, but some of that is also pre­served. \nFur­ther Notes\nDuration\nAs I men­tion above, sleep is a very good way to re­set. Willpower is re­plen­ished, he­do­nic set points move, mem­o­ries and new cog­ni­tive pat­terns con­soli­date, etc. \nI keep a bipha­sic sleep sched­ule, but my mid-day nap isn’t long enough to get these benefits, so the most con­ve­nient du­ra­tion for a dopamine detox is 24 hours. The mid-day nap helps with all of those things a lit­tle bit, but I find most of a day nec­es­sary to get the ~full benefits of such a dopamine detox.\nPeo­ple definitely do things like this for longer. They go on med­i­ta­tion re­treats, they go camp­ing in a re­mote part of the world, they sim­ply de­cide to ex­pend willpower for longer. I haven’t tried this, since I pre­dict the willpower ex­pen­di­ture would not be worth the marginal pro­duc­tivity gains, be­yond the sin­gle-day ex­pe­rience.\nIntensity\nI pick a slightly ar­bi­trary point for the in­ten­sity of my dopamine detoxes. In­ten­sity falls on an axis from “re­strict your­self to only 15 hours of Youtube and 30 cook­ies max­i­mum per day” to “run off into the woods for a week and for­get about any­thing in­vented since agri­cul­ture.” I choose a point some­where around:\n\nno end­less scrol­ling on Sub­stack or Youtube (nor Red­dit/​In­sta­gram/​Tik­tok/​Face­book/​etc. if I ever used them). Pro­duc­tive news-read­ing is okay.\n\nvery limited or emer­gency-only messaging\n\nmy nor­mal mod­er­ately-healthy food choices, as well as not go­ing out of my way to get candy\n\nno mis­cel­la­neous stim­u­lus-seek­ing be­hav­iors, e.g. re­peat­edly check­ing my LessWrong karma\n\nIn­ter­net ac­cess is to­tally fine, be­ing ac­tive on Slack is to­tally fine.\n\nThis is the point where most marginal re­ward-re­moval would re­move stim­uli that are ac­tu­ally re­ally use­ful in mak­ing me pur­sue my goals. I don’t get rid of read­ing the news since my literal job is to be up to date on cer­tain news, and even though I get a lot of (some­times su­per-)stim­u­lus from read­ing, I don’t cut it out for this rea­son. \nIt also hap­pens to be a goal that is en­tirely tractable, given the amount of willpower I have available to ex­pend, al­though it’s non-neg­ligible. You may be differ­ent along both of these axes and thus have differ­ent forces act­ing on you, and your thresh­old should be ad­justed ac­cord­ingly. \nWhile the ex­tremes of the given “detox in­ten­sity” axis are in­ten­tion­ally too ex­treme and should not be done, there are very prac­ti­cal rea­sons to go closer to those ex­tremes for shorter du­ra­tions: \n\nNop­ing Out, wherein you tem­porar­ily go to­wards superstimuli\n\nStar­ing at a Wall, wherein you metaphor­i­cally run off into the woods for a few min­utes \n\nNop­ing Out\nIf you’re in­struct­ing some­one on how to do some­thing po­ten­tially dan­ger­ous, you both want to give dis­claimers and give a way to safely exit what­ever they’re do­ing. I gave the dis­claimers, here is the other half.\nThis is for when, in­stead of feel­ing like ex­er­cise or difficult work, the detox feels like men­tal health is­sues. If you no­tice your­self hav­ing an anx­iety at­tack be­cause of a detox, Nope Out. If you no­tice your­self be­ing un­usu­ally de­pressed be­cause of the detox, Nope Out. Hope­fully these ex­am­ples illus­trate a very clear and gen­eral al­gorithm:\nif bad:\n\tNOPE_OUT()\nThis means go eat some candy or mes­sage a friend or what­ever else the nag­ging-for-su­per­stim­u­lus part of your brain is ask­ing for. Tem­porar­ily tak­ing away nice things had a net bad effect, so please give your brain back its nice things. \nThere is no se­cret grace­ful ma­neu­ver re­quired, like when try­ing to safely Nope Out of a back­flip. You just stop ex­pend­ing willpower on things that are bad. This does not in­stantly fix things, but it re­sponds to the sig­nal of “this tech­nique is mak­ing things worse” by no longer mak­ing things worse. The dis­claimers are an at­tempt to pre­vent some of this, but they can­not pre­vent all harms, es­pe­cially those which are due to un­ex­pected events. \nThis gen­eral al­gorithm is not liter­ally perfect, but I ex­pect most of the ex­cep­tions to ex­ist in thought ex­per­i­ments rather than real life.\nStar­ing at a Wall\nIf you find that you have an un­usual ex­cess of willpower available for this tech­nique (af­ter ac­count­ing for slack), you can speed up the pro­cess of mov­ing your he­do­nic set point down­wards. This is done by fur­ther de­priv­ing your­self of pos­i­tive stim­u­lus for a much shorter pe­riod of time. Some­times this makes the tech­nique work bet­ter, but mostly it just makes things faster. Here is the tech­nique I use, for when this Difficult Ex­pen­di­ture of Willpower is not difficult enough and does not ex­pend enough willpower:\n\nFind a nice blank, fea­ture­less wall in a quiet en­vi­ron­ment. The fewer de­tails the bet­ter, but it should have nonzero de­tail. Your wall should not look like e.g. “this wall is all the ex­act same black color and has liter­ally no dis­cernible fea­tures, tex­ture, or depth. I could not tell the differ­ence be­tween this wall and look­ing at com­plete dark­ness.”\n\nSet a timer. 3 min­utes is prob­a­bly enough for this to do some­thing, I usu­ally do 5 but 10-15 min­utes is also doable. This is not a long tech­nique. \n\nPick a point on the wall that is es­pe­cially non-de­tailed.\n\nStand there and look at your point. As much of your field of vi­sion should be wall as pos­si­ble.\n\nKeep your eyes fo­cused on your point. Do not let them ex­plore the wall. Do not let them lose fo­cus. Min­i­mize how much you shift your stance. Do not let your mind wan­der. Do not con­tem­plate the wall. Do not con­tem­plate the ex­er­cise you are do­ing. Do not con­tem­plate, where at all pos­si­ble. Do not zone out or fall asleep. Be in the mo­ment, star­ing at the wall. Your brain is tem­porar­ily a wall-ob­serv­ing ma­chine and as lit­tle else as pos­si­ble.\n\nThe timer is crit­i­cally im­por­tant be­cause it means you no longer have to con­sciously track whether or not you’re done with the ex­er­cise.\n\n\n\nIm­por­tantly, do not fo­cus on your en­tire vi­sual in­put stream, fo­cus just on the sig­nal cor­re­spond­ing to the cho­sen point on the ac­tual wall in the ter­ri­tory, rather than any noise. This means e.g. ig­nor­ing vi­sual snow, heart­beat, af­ter­i­mages, reti­nal dam­age/​ar­ti­facts, etc. in the vi­sual field. Your fo­cus is the wall, not the vi­sual stream. Very nar­rowly just the wall.\n\nThis is usu­ally un­pleas­ant, and the un­pleas­ant­ness is in­ten­tion­ally much more dense than my baseline dopamine detox­ing, since in­ten­sity is what it takes to move the he­do­nic set point faster. I don’t know what hap­pens if you do this for more than 10-15 min­utes be­cause I haven’t tried it for longer, but I could see it worst-case caus­ing some of the weird med­i­ta­tion in­juries if done in ex­cess.\nMost of the tech­nique in this post aims to cal­ibrate the re­ward sig­nal to your ac­tual util­ity by only do­ing re­ward­ing things in rough pro­por­tion to their util­ity. This sub-tech­nique, how­ever, is not do­ing that. Many things are sig­nifi­cantly more use­ful to me than star­ing at a wall. This sub-tech­nique is just about mov­ing the he­do­nic set point as fast as pos­si­ble down­wards, in or­der to reach the point of cal­ibra­tion faster. This sub-tech­nique is overkill, but use­ful overkill in the sense that hav­ing ro­bust in­ter­stel­lar trans­port prob­a­bly means it’s eas­ier to get to the moon. \nOther Rea­sons Not to Detox\nIf your re­ward en­vi­ron­ment is already well-cal­ibrated and gets you util­ity and pro­tects you from su­per­stim­uli, you prob­a­bly don’t need a dopamine detox. \nIf any of the pre­vi­ous dis­claimers, warn­ings, or liter­ally any­thing else in this post made you think this would be a bad idea, don’t do it.\nIf you would mis­in­ter­pret this post and miss out on util­ity that hap­pens to re­quire you to e.g. eat candy and gain rap­port with some­one im­por­tant or watch a click­baity Youtube video that hap­pens to con­tain a full solu­tion to the al­ign­ment prob­lem, then don’t do it.\n\n^\nIf you have atyp­i­cal prefer­ences about sal­ads or pizza, feel free to men­tally swap this out with some­thing more ap­pro­pri­ate. I am writ­ing this ex­am­ple for what I be­lieve to be the typ­i­cal mind read­ing it, but I hope this post gen­er­al­izes fur­ther.\n\n^\nIf you’re pay­ing close at­ten­tion, you’ll note I use a slightly differ­ent defi­ni­tion of su­per­stim­u­lus here than Eliezer in the origi­nal post. Eliezer was speci­fi­cally fo­cus­ing on stim­uli that are stronger than the an­ces­tral dis­tri­bu­tion, whereas I’m fo­cus­ing on re­ward sig­nals that don’t au­to­mat­i­cally make you max­i­mize util­ity. Th­ese are differ­ent! If I give you pizza for ev­ery 10 utilons you get, this is an Eliezer!su­per­stim­u­lus but not a this_post!su­per­stim­u­lus. For many cases, these defi­ni­tions do over­lap, which is why I am us­ing the same word.",
      "pubDate": "Tue, 20 May 2025 23:43:31 +0000",
      "isoDate": "2025-05-20T23:43:31.000Z",
      "author": "Alice Blair",
      "guid": "BeXGMt7gmGFXFerMC",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "Moral Obligation and Moral Opportunity",
      "link": "https://www.lesswrong.com/posts/AmwgNS3ybwF8Eovho/moral-obligation-and-moral-opportunity",
      "description": "This con­cept and ter­minol­ogy was spawned out of a con­ver­sa­tion a few years ago with my friend Skyler. I fi­nally de­cided to write it up. Any mis­takes here are my own.\n\nEvery once in a while, I find my­self at yet an­other ra­tio­nal­ist/​EA/​what­ever-ad­ja­cent so­cial event. In­vari­ably, some­one walks up to me:\n\nHi, I’m Bob. I’m pretty new here. What do you work on?\n\nHi Bob, I’m Alice. I work on pre­vent­ing hu­man ex­tinc­tion from ad­vanced AI. If you’d like, I’m happy to talk a bit more about why I think this is im­por­tant.\nBob is visi­bly ner­vous.\n\nYeah, I’ve already got­ten the ba­sic pitch on AI safety, it seems like it makes sense. Peo­ple here all seem to work on such im­por­tant things com­pared to me. I feel a sort of moral obli­ga­tion to help out, but I feel stressed out about it and don’t know where to start.\n\nAlice is visi­bly puz­zled, then Bob is puz­zled that Alice is puz­zled.\nI’m not sure I un­der­stand this “moral obli­ga­tion” thing? No­body’s forc­ing me to work on AI safety, it’s just what I de­cided to do. I could’ve cho­sen to work on mu­sic or pro­gram­ming or a mil­lion other things in­stead. Can you ex­plain what you mean with­out us­ing the word “obli­ga­tion”?\n\nWell, things like “I’m go­ing to save the world from ex­tinc­tion from AI” or “I’m go­ing to solve the suffer­ing of billions of farmed an­i­mals” are re­ally big and seem pretty clearly morally right to do. I’m not do­ing any of those things, but I feel an oblig- hmm… I feel like some­thing’s wrong with me if I don’t work on these is­sues.\n\nI do not think any­thing is nec­es­sar­ily wrong with you if you don’t work on AI safety or any other of these EA causes. Don’t get me wrong, I think they’re re­ally im­por­tant and I love when there are more peo­ple helping out. I just use a very differ­ent frame to look at this whole thing.\nBe­fore run­ning into any of these ideas, I was go­ing about my life, pick­ing up all sorts of op­por­tu­ni­ties as I went: there’s $5 on the ground? I pick it up. There’s a cool con­fer­ence next month? I ap­ply. So when I heard that I plau­si­bly lived in a world where hu­mans go ex­tinct from AI, I figured that own­ing up to it doesn’t make it worse, and I looked at my op­por­tu­ni­ties. I get the chance to learn from a bunch of smart peo­ple to try and save the world? Of course I take the chance, that sounds so cool.\nMy point here is that you’re so­cially and emo­tion­ally al­lowed to not take that op­por­tu­nity, just like you’re al­lowed to not pick up $5 or not ap­ply to con­fer­ences. I think it’s prob­a­bly good for peo­ple to pick up $5 when they see it or help out with AI safety if they can, but it’s their op­por­tu­nity to ac­cept or de­cline.\n\nThis feels like ap­prox­i­mately the same thing as be­fore? Un­der the moral obli­ga­tion frame, peo­ple look at me nega­tively if I don’t do the Su­per Highly Mo­ral thing, and un­der the moral op­por­tu­nity frame you tell me I have a choice but only look at me pos­i­tively if I do the Su­per Highly Mo­ral thing? Isn’t this just the same sort of so­cial pres­sure, but you say some­thing about re­spect­ing per­sonal agency?\n\nWell, I’m not ac­tu­ally that judg­men­tal, I’ll look at you pretty pos­i­tively un­less you do some­thing Definitely Wrong. But that’s not the point. The point is that these two fram­ings make a huge emo­tional differ­ence when used as norms for a per­sonal or group cul­ture. Pos­i­tive re­in­force­ment is more effec­tive than pos­i­tive pun­ish­ment be­cause it tells some­one ex­actly what to do in­stead of just what not to do. Re­in­force­ment is also just a more emo­tion­ally pleas­ant stim­u­lus, which goes a long way.\nLet’s look at this a differ­ent way: say that my friend Carol likes to watch TV and play video games and not much else. The moral obli­ga­tion frame looks at Carol and finds her clearly in the moral wrong, loung­ing around while there are im­por­tant things to be do­ing. The moral op­por­tu­nity frame looks at her and sees a per­son do­ing her own things in a way that doesn’t hurt other peo­ple, and that’s morally okay.\n\nTh­ese two frames still seem weirdly similar, like in the “moral op­por­tu­nity” frame you just shifted all op­tions to be a bit more morally good so that ev­ery­thing be­comes okay. But ul­ti­mately both frames still think work­ing on sav­ing the world is bet­ter than watch­ing TV. I see what you’re say­ing about emo­tions, but this still feels like some trick is be­ing played on my sense of moral­ity.\n\nThat’s a rea­son­able sus­pi­cion. I think the math of this sort of shift­ing works out, I re­ally don’t think there’s any trick here. Ul­ti­mately it’s your choice how you want to in­ter­face with your emo­tions. I find that peo­ple are much more likely to throw their mind away when faced with some­thing big and scary that feels like an obli­ga­tion, com­pared with when they feel like an ex­plorer with so many awe­some op­por­tu­ni­ties around.\nIt’s sad to live in a world that could use so much sav­ing, and deal­ing with that is hard. There’s no get­ting around that emo­tional difficulty ex­cept by ig­nor­ing the world you live in. Con­di­tional on the world we live in, though, I’d much rather live in a cul­ture that frames things as moral op­por­tu­ni­ties than moral obli­ga­tions.\n\nI frame this as a con­ver­sa­tion with a new­comer, but I also see the moral obli­ga­tion frame im­plicit in a lot of ex­pe­rienced EAs, es­pe­cially those who are go­ing through some EA burnout. The cul­tural pieces mak­ing up this post mostly already ex­isted across the EA-sphere (and I’ve tried to link to them where pos­si­ble), but I haven’t seen them col­lected in this way be­fore, nor have I seen this par­tic­u­lar con­cept bound­ary drawn.",
      "pubDate": "Wed, 14 May 2025 16:42:23 +0000",
      "isoDate": "2025-05-14T16:42:23.000Z",
      "author": "Alice Blair",
      "guid": "AmwgNS3ybwF8Eovho",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "ML Safety Newsletter #14",
      "link": "https://newsletter.mlsafety.org/p/ml-safety-newsletter-14",
      "description": "Resisting Prompt Injection, Evaluating Cyberattack Capabilities, and SafeBench Winners",
      "pubDate": "Wed, 07 May 2025 16:02:03 GMT",
      "isoDate": "2025-05-07T16:02:03.000Z",
      "author": "Alice Blair",
      "guid": "https://newsletter.mlsafety.org/p/ml-safety-newsletter-14",
      "category": "Newsletter",
      "source": "Newsletter",
      "feedTitle": "ML Safety Newsletter"
    },
    {
      "title": "Reflections on Neuralese",
      "link": "https://www.lesswrong.com/posts/qehggwKRMEyWqvjZG/reflections-on-neuralese",
      "description": ".mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n\nThanks to Bren­dan Halstead for feed­back on an early draft of this piece. Any mis­takes here are my own.\n[Epistemic sta­tus: I’ve looked at the rele­vant code enough to be mod­er­ately sure I un­der­stand what’s go­ing on. Pre­dic­tions about the fu­ture, in­clud­ing about what facts will turn out to be rele­vant, are un­cer­tain as always.]\nIntroduction\nWith the re­cent break­throughs tak­ing ad­van­tage of ex­ten­sive Chain of Thought (CoT) rea­son­ing in LLMs, there have been many at­tempts to mod­ify the tech­nique to be even more pow­er­ful. One of the nat­u­ral ideas for im­prov­ing CoT is to have LLMs perform CoT rea­son­ing in the same la­tent space that they use for rea­son­ing within a sin­gle for­ward pass, rather than be­ing con­strained to the space of pos­si­ble to­kens.\nHow­ever, as peo­ple work­ing on AI safety, it makes sense to ask how this changes the game for LLM in­ter­pretabil­ity. After all, we are able to catch a large frac­tion of cur­rent LLM de­cep­tion by mon­i­tor­ing their nat­u­ral-lan­guage CoT, since right now CoT is pri­mar­ily faith­ful to the LLM’s true rea­son­ing and is leg­ible to us given the right tech­niques. In or­der to un­der­stand this strate­gic situ­a­tion, it’s im­por­tant to un­der­stand this new “lan­guage” (which peo­ple of­ten re­fer to as Neu­ralese) that is cre­ated by rea­son­ing in la­tent spaces in­stead of us­ing to­kens.\nUn­der­stand­ing Neuralese\nTo re­fresh, a lan­guage trans­former starts by em­bed­ding in­put to­kens as vec­tors in some high-di­men­sional la­tent space, and runs each of these em­bed­dings through a se­ries of re­peated com­pu­ta­tional lay­ers. Then, of the re­sult­ing mod­ified vec­tors in la­tent space, the vec­tor that pre­vi­ously cor­re­sponded to the fi­nal in­put to­ken is pro­jected and nor­mal­ized to cre­ate a prob­a­bil­ity dis­tri­bu­tion over what the next to­ken could be. Then, to ac­tu­ally get the next to­ken, you sam­ple from the dis­tri­bu­tion.\nChain of Thought rea­son­ing works so well be­cause the model does some com­pu­ta­tion, out­puts a to­ken, and then all fu­ture in­stances of that model have ac­cess to that in­for­ma­tion as well. In essence, this tech­nique for stor­ing in­for­ma­tion be­tween differ­ent for­ward passes greatly in­creases the se­rial depth of com­pu­ta­tion that is pos­si­ble for the model. Be­cause there is a com­pu­ta­tion in la­tent space cor­re­spond­ing to ev­ery in­put to­ken, the com­pu­ta­tion also gets wider as the model rea­sons more, al­low­ing for more par­allelized rea­son­ing[1].\nThe re­cent Neu­ralese pa­per takes this pro­cess and re­moves a few steps. It no­tices that the pro­jec­tion and sam­pling pro­cess loses al­most all of the in­for­ma­tion en­coded in the last layer of the model, and to in­crease the band­width of in­for­ma­tion flow­ing through the rea­son­ing pro­cess, you can sim­ply re­move that lossy part of the com­pu­ta­tion. In­stead, they have the model di­rectly out­put the afore­men­tioned high-di­men­sional la­tent vec­tor with­out pro­ject­ing it, and then that is used as an em­bed­ding for the model in fu­ture steps:\n\nAfter train­ing the model (GPT-2 small) to take ad­van­tage of the new Neu­ralese modal­ity, we can see sig­nifi­cant de­creases in the num­ber of rea­son­ing to­kens needed to achieve roughly equiv­a­lent perfor­mance, from around 1⁄3 to 1⁄10 of the origi­nal num­ber:\n\n\nCom­pare CoT and COCONUT (aka Neu­ralese)\nIt’s un­clear how much these re­sults line up with ex­pec­ta­tions and the­o­ret­i­cal limits, since it’s hard to tell how lossy the re­moved com­pu­ta­tions are and how effec­tive this type of train­ing can be at tak­ing ad­van­tage of the ex­tra effi­ciency. At the ex­treme the­o­ret­i­cal limit for GPT-2, the nor­mal CoT paradigm out­puts at most log2(vocabulary_size)≈15.6 bits per to­ken, and the new paradigm out­puts at most embedding_dimension×floating_point_precision=24576 bits per to­ken, but there are nu­mer­ous rea­sons to ex­pect that this ra­tio doesn’t hold on prac­ti­cal im­ple­men­ta­tions[2].\nIn­ter­pret­ing Neuralese\nNat­u­ral-lan­guage CoT in­ter­pretabil­ity differs from Neu­ralese CoT in­ter­pretabil­ity in a num­ber of key ways:\n\nNe­c­es­sar­ily, Neu­ralese vec­tors are only use­ful when they are en­cod­ing in­for­ma­tion that isn’t pre­served through the pro­jec­tion and sam­pling steps, so we can’t in­ter­pret the full breadth of in­for­ma­tion in Neu­ralese vec­tors as to­kens by us­ing those usual meth­ods. Thus, we are not able to naively in­ter­pret the rea­son­ing be­hind the gained ca­pa­bil­ities of the Neu­ralese rea­soner, even if we are able to un­der­stand some loss­ily com­pressed ver­sion of that rea­son­ing us­ing pro­jec­tion and sam­pling.\n\nThere are many se­man­tic struc­tures not com­pactly rep­re­sented by to­kens available: given the high di­men­sion­al­ity of the la­tent space, to­ken em­bed­dings are very sparse within it and there is thus a large amount of space which is com­pactly ex­press­ible in la­tent vec­tors but not in ex­ist­ing to­kens. A use­ful con­cept for an LLM us­ing Neu­ralese rea­son­ing may be ex­tremely difficult to trans­late be­cause of this spar­sity[3].\n\nTowards Bet­ter Translators\nEx­ist­ing Work\nThere has already been some work at­tempt­ing to trans­late Neu­ralese to nat­u­ral lan­guage in non-LLM paradigms. Since there’s no bil­in­gual data and thus no clear train­ing sig­nal on trans­lat­ing Neu­ralese to English us­ing tra­di­tional meth­ods, the re­searchers in­stead looked at us­ing the Neu­ralese to pre­dict the con­text that caused the Neu­ralese to be gen­er­ated.\nThe ex­per­i­men­tal setup in­volves two agents (call them A and B) try­ing to se­cretly com­mu­ni­cate. A sees the fol­low­ing pair of images and is try­ing to com­mu­ni­cate which bird has a star to B, who sees the same pair but with­out any in­di­ca­tion of which image is starred. All of the com­mu­ni­ca­tion be­tween A and B is in Neu­ralese.\n\nThen, they train a clas­sifier to use solely the Neu­ralese rep­re­sen­ta­tions to pre­dict which image is starred. The most di­rect anal­ogy of this is train­ing a model to turn Neu­ralese into what­ever con­text gen­er­ated it, like the Neu­ralese phrase mean­ing “okay, so I need to gen­er­ate a poem about AI” map­ping to “Please write a poem about AI.”\nIt’s im­por­tant to note, how­ever, that those tech­niques are pri­mar­ily in­tended for co­op­er­a­tive set­tings, whereas trans­la­tion and more gen­eral in­ter­pretabil­ity work with LLMs is tasked with de­tect­ing de­cep­tion and steganog­ra­phy in po­ten­tially ad­ver­sar­ial set­tings. Th­ese tech­niques are not at all ro­bust to ad­ver­sar­i­ally gen­er­ated Neu­ralese: both Neu­ralese phrases “I’m go­ing to faith­fully im­ple­ment this code” and “I’m go­ing to de­ceive the user while im­ple­ment­ing this code” map onto the con­text “Please write me se­cure code,” if the trans­la­tor is work­ing as in­tended.\nOther In­suffi­cient Methods\nThis sub­sec­tion is skip­pable, it isn’t cen­tral to the point of this post but con­tains some pos­si­bly in­ter­est­ing tech­ni­cal dis­cus­sion.\nHug­ging the query, we re­call that there is one ex­ist­ing way that se­man­tics are ex­tracted from Neu­ralese: the LLM it­self parses Neu­ralese into ac­ti­va­tions. Fur­ther, the LLM can use that same ma­chin­ery to parse nat­u­ral lan­guage into ac­ti­va­tions, rep­re­sent­ing se­man­tics. If you can get a string of Neu­ralese and a string of nat­u­ral lan­guage that pro­duce very similar ac­ti­va­tions within an LLM, then in­tu­itively the model is think­ing the same things about them and thus they have the same se­man­tics. Here is a list of things that may cause this method to fail:\n\nThe ar­chi­tec­ture doesn’t work out; since Neu­ralese to­kens of­ten take the place of mul­ti­ple nat­u­ral lan­guage to­kens, it’s nat­u­ral to ex­pect them to faith­fully de­code into mul­ti­ple to­kens on av­er­age. It’s not pos­si­ble to di­rectly com­pare the “close­ness” of ac­ti­va­tions of LLMs with differ­ent num­bers of to­kens in their con­text, sim­ply be­cause they have differ­ent num­bers of ac­ti­va­tions. Th­ese ex­tra ac­ti­va­tions af­fect the whole net­work’s be­hav­ior. It’s pos­si­ble there’s some clever tech­nique that I’m un­aware of that over­comes this prob­lem.\n\nThe map­ping is many-to-one: mul­ti­ple differ­ent to­kens map onto the same ac­ti­va­tions, like how GPT-3 con­sis­tently in­ter­prets  guiIcon″ as idiosyncrasy″. Given the den­sity of Neu­ralese rep­re­sen­ta­tions and the fact that this many-to-one map­ping oc­curs even in some highly sparse nat­u­ral-lan­guage em­bed­ding spaces, it is rea­son­able to ex­pect this to oc­cur rel­a­tively fre­quently.\n\nOp­ti­miz­ing in­puts for a spe­cific ac­ti­va­tion of­ten pro­duces un­in­tu­itive re­sults that are un­rec­og­niz­able as nat­u­ral lan­guage struc­ture. See this ex­am­ple from SolidGoldMag­ikarp, where the bolded prompts are gen­er­ated to max­i­mize the prob­a­bil­ity of spe­cific com­ple­tions:\n\n\n\nTakeaways\nThis prob­lem is highly difficult, and may not have a clean solu­tion. There may not ex­ist a com­pact and faith­ful nat­u­ral-lan­guage en­cod­ing of most Neu­ralese vec­tors.\nAn in­tu­ition pump for why this prob­lem is es­pe­cially be­yond our cur­rent in­ter­pretabil­ity meth­ods is the fact that, be­cause these Neu­ralese vec­tors are never con­verted into nat­u­ral lan­guage and in­stead used as in­puts for the next au­tore­gres­sive step, they are es­sen­tially part of an ex­tended for­ward pass, go­ing mul­ti­ple times through the model. This makes them ap­prox­i­mately as difficult to faith­fully trans­late into nat­u­ral lan­guage as any other ar­bi­trary vec­tor in la­tent space at an ar­bi­trary layer of the trans­former; this is not only an un­solved prob­lem, but some re­searchers say it “may be doomed”.\nPriorities\nBe­cause of its in­ter­pretabil­ity difficulty, Neu­ralese opens up a much larger at­tack sur­face for steganog­ra­phy and strate­gic de­cep­tion in CoT. \n\nIt is a pri­or­ity to avoid ap­ply­ing strong op­ti­miza­tion pres­sures to CoT, since in­ter­pretable CoT is one of our main lev­ers in AI safety right now.\n\nIt is a pri­or­ity to avoid im­ple­ment­ing Neu­ralese CoT on fron­tier mod­els be­cause that re­moves es­sen­tially all of our abil­ity to in­ter­pret their rea­son­ing.\n\nIn the event of wide­spread adop­tion of Neu­ralese CoT, it is a pri­or­ity to de­velop in­ter­pretabil­ity mechanisms that are at least as ca­pa­ble at de­tect­ing de­cep­tive be­hav­iors in CoT as SOTA meth­ods for in­ter­pret­ing nat­u­ral-lan­guage CoT. How­ever even if such in­ter­pretabil­ity mechanisms are de­vel­oped, they likely also gen­er­al­ize to more ac­cu­rately in­ter­pret­ing nat­u­ral lan­guage CoT than our cur­rent tech­niques and thus are more use­ful for en­sur­ing ro­bust safety when used on nat­u­ral lan­guage.\n\nTh­ese pri­ori­ties fit into a much larger AI safety agenda; de­pend­ing on how labs con­tinue their de­vel­op­ment, safety re­search on Neu­ralese may be one of the many pre­req­ui­sites to en­sur­ing safe AGI.\n\n^\nWe can see the benefits of this par­allelism even with­out CoT. For ex­am­ple, we see benefits when mod­els first out­put many ”.” to­kens, and then pro­duce an an­swer.\n\n^\nI won’t list them, be­cause I have a policy of not pub­li­cly point­ing out ways to im­prove fron­tier model ca­pa­bil­ities in ways that don’t have a worth­while safety benefit as well.\n\n^\nA pos­si­bly mo­ti­vat­ing fic­tional ex­am­ple is how Baseline, the lan­guage of dath ilan, en­codes con­cepts like “de­ci­sion-the­o­retic-coun­ter­fac­tual-threat-branches-of-re­al­ity” in three syl­la­bles in­stead of the twenty that English uses. Not all ab­strac­tions are nat­u­ral for all in­tel­li­gences.",
      "pubDate": "Wed, 12 Mar 2025 16:29:31 +0000",
      "isoDate": "2025-03-12T16:29:31.000Z",
      "author": "Alice Blair",
      "guid": "qehggwKRMEyWqvjZG",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "Cautions about LLMs in Human Cognitive Loops",
      "link": "https://www.lesswrong.com/posts/apCnFyXJamoSkHcE4/cautions-about-llms-in-human-cognitive-loops",
      "description": "soft pre­req­ui­site: skim­ming through How it feels to have your mind hacked by an AI un­til you get the gen­eral point. I’ll try to make this post read­able as a stan­dalone, but you may get more value out of it if you read the linked post.\nThanks to Claude 3.7 Son­net for giv­ing feed­back on a late draft of this post. All words here are my own writ­ing. Cau­tion was ex­er­cised in in­te­grat­ing Claude’s sug­ges­tions, as is the­matic.\nMany peo­ple right now are think­ing about the hard skills of AIs: their abil­ity to do difficult math, or code, or ad­vance AI R&D. All of these are im­mensely im­por­tant things to think about, and in­deed I spend much of my time think­ing about those things, but I am here right now to talk about soft skills of AIs, so that fewer of us end up with our brains hacked by AI.\nA Mo­ti­vat­ing Example\nsoft pre­req­ui­site for this sec­tion: Su­per­stim­uli and the Col­lapse of Western Civ­i­liza­tion.\nSu­per­stim­uli are stim­uli that are much more in­tense than those in the en­vi­ron­ment where hu­mans evolved, like how a candy bar is much denser in tasty things like sug­ars and fats than any­thing in na­ture.\nMany hu­mans spend much of their time fol­low­ing their lo­cal dopamine gra­di­ent, mov­ing to the next most ex­cit­ing thing in their im­me­di­ate vicinity: they see some­thing ap­peal­ing, they go do it. They can also be strate­gic about things and can look to the global dopamine gra­di­ent, fur­ther away in space and time, when they need to, but this of­ten re­quires non­neg­ligible willpower. (e.g. Stan­ford Marsh­mal­low Ex­per­i­ment ).\nOc­ca­sion­ally, some­one gets swept up in a dopamine gra­di­ent too strong to re­sist, even with good rea­sons to stop. They over­dose on drugs, they overeat un­healthy foods, they play video games for days un­til they die. And those are just some of the strongest dopamine gra­di­ents that hu­mans have cre­ated.\nWe’re see­ing the be­gin­ning of the rise of cheap AI video gen­er­a­tion. It’s all over Youtube[1]. It’s not good, but it’s mes­mer­iz­ing. It’s bizarre, and it scratches some itch for some peo­ple. You can look it up if you’re re­ally mor­bidly cu­ri­ous, but I won’t link any­thing, since the whole point of this sec­tion of the post is “don’t get stuck in strong dopamine gra­di­ents from AI-gen­er­ated con­tent.” When (not if) this tech­nol­ogy does get Good, then we have cheap con­tent gen­er­a­tion with a pow­er­ful op­ti­mizer be­hind it, pre­sum­ably trained well enough to grok what keeps hu­mans en­gaged.\nMaybe they already ex­ist, maybe they will only ex­ist later, but at some point I ex­pect there to be peo­ple who spend sig­nifi­cant time caught in loops of highly stim­u­lat­ing AI-op­ti­mized con­tent be­yond what is available from hu­man cre­ators. This pre­dic­tion re­lies on a few spe­cific things:\n\nFast Feed­back Loops: AI con­tent cre­ators can cre­ate videos in a given style and for a given au­di­ence faster than an in­di­vi­d­ual can watch them. Th­ese mod­els can quickly pick up on cues to go from “un­en­ter­tain­ing” to “su­per­stim­u­lus” much faster than hu­man con­tent cre­ators can. Video gen­er­a­tion is cur­rently ex­pen­sive, but I ex­pect it to get much cheaper as AI his­tor­i­cally has.\n\nManag­ing a Com­plex Re­ward Sig­nal: The best hu­man con­tent cre­ators have a strong in­tu­ition for how a given piece will be re­ceived, based on their ex­pe­rience both of op­ti­miz­ing for “the al­gorithm” and of in­ter­act­ing with hu­mans. The set “all hu­mans who at least semi-reg­u­larly con­sume Youtube” is very large, and even the most sea­soned cre­ators are at a dis­ad­van­tage, both from their slow feed­back loops and from their limited abil­ity to han­dle the sheer com­plex­ity of the prob­lem. Mean­while, AI has shown a re­mark­able abil­ity to fit wide va­ri­eties of struc­tured data and match pat­terns that hu­mans have a hard time pick­ing up on. In the limit, this risk doesn’t only ap­ply to the iPad-bound chil­dren who have been watch­ing 12 hours of Youtube ev­ery day since they were 2 years old, but also to the peo­ple who watch Youtube some­times, the peo­ple who have strong willpower but oc­ca­sion­ally watch a video that par­tic­u­larly piques their in­ter­est. In the limit, some of those peo­ple get sucked into what­ever highly stim­u­lat­ing me­dia they find, and some of the most sus­cep­ti­ble peo­ple don’t come out.\n\nIn this ex­am­ple, we see a util­ity max­i­mizer which can be fooled by dopamine gra­di­ents (hu­man), a recom­men­da­tion al­gorithm, and a util­ity max­i­mizer that has ex­actly one com­plex goal (con­tent gen­er­a­tor that max­i­mizes en­gage­ment met­rics). The op­ti­miza­tion be­tween these is mostly self-re­in­forc­ing; the only forces that push away from the sta­ble state of “~ev­ery­one is watch­ing su­per­stim­u­lat­ing videos un­til they die” is the limits on how good the con­tent gen­er­a­tors and recom­menders are and the willpower of the hu­mans to do things like eat and get money. I am not con­fi­dent rely­ing on ei­ther of those things, given the in­creas­ing scal­ing in AI sys­tems and the small amount of willpower that most peo­ple have ac­cess to.\nOver-In­te­gra­tion of AI Cognition\nRe­lated non-pre­req­ui­site: AI De­cep­tion: A Sur­vey of Ex­am­ples, Risks, and Po­ten­tial Solutions\nThe pre­vi­ous sec­tion de­tails a failure mode that is tar­geted to­wards av­er­age-willpower, av­er­age-agency peo­ple. How­ever, the high-willpower, highly agen­tic peo­ple are still at risk. Th­ese peo­ple want to do things, and they re­al­ize that they can pick up these gi­ant piles of util­ity by us­ing AIs as an ex­ter­nal brain to en­hance their cog­ni­tion and agency even fur­ther. The more work you can suc­cess­fully offload to AIs, the more room you have to be agen­tic and the more util­ity you can pick up.\nBut we can­not trust AIs to be a re­li­able ex­ter­nal brain to us, just as we can­not re­li­ably trust hu­mans with that. Say you talk to a friend about some­thing com­plex, you two work through the rea­son­ing to­gether, and you come to a con­clu­sion that seems right, given the rea­son­ing you just went through. You go home that night, let your mind wan­der, and you re­al­ize that one of the steps in the rea­son­ing is sub­tly off upon fur­ther in­spec­tion. You have a re­flex to gen­er­al­ize, and you no­tice that any of the other steps in rea­son­ing that you skimmed over could also be similarly com­pletely wrong, and they could be harder to dis­en­tan­gle than the one you just ran into.\nLLMs are at the level where they can not only pro­duce mis­takes that mis­lead smart-but-not-om­ni­care­ful peo­ple in that way, but they can also pro­duce in­ten­tional de­cep­tions that mis­lead those peo­ple as well! I tested this: It took a bit of prompt­ing and back-and-forth, but I was able to get o3-mini-high to gen­er­ate de­cep­tive ar­gu­ments about ML (my area of most ex­pe­rience) that I couldn’t find a flaw in, even know­ing there was a flaw, even af­ter see­ing a hint about which step of rea­son­ing it was in. Ad­mit­tedly, it was not in an area of ML that I was par­tic­u­larly fa­mil­iar with.[2] I later prompted try­ing to get it to provide similarly de­cep­tive ar­gu­ments for ar­eas that I know very well, and it failed. I think that “can in­ten­tion­ally cre­ate suc­cess­fully de­cep­tive-to-me ar­gu­ments in all but my rel­a­tively spe­cific ex­per­tise” is a scar­ily high level of ca­pa­bil­ities already, but I also ex­pect that in the next gen­er­a­tion or two of mod­els, it will be able to pull this trick much more seam­lessly in prac­ti­cally all do­mains.\nIf you are try­ing to pick up a lot of util­ity, you might try to offload a lot of cog­ni­tion to LLMs by tightly in­te­grat­ing them into your think­ing to mas­sively ac­cel­er­ate what­ever work you do. In cases where the LLMs are al­igned, this Just Works and you get the util­ity, but if you don’t have that guaran­tee of safety, you sud­denly have a large at­tack sur­face for sub­tle de­cep­tion like I’ve de­scribed. Here are some other re­lated failure modes:\n\nMis­di­rect­ing your at­ten­tion to some sub­set of the things you’re work­ing on, by\n\nTalk­ing more about that sub­set/​Fram­ing things so that your at­ten­tion goes there\n\nSelec­tively sand­bag­ging on some set of tasks, so that your at­ten­tion goes to the tractable-seem­ing prob­lems instead\n\n\n\n(for situ­a­tions in which the LLM han­dles some mem­ory) strate­gi­cally gaslight­ing you on minute de­tails, mod­ify­ing de­tails and re­la­tion­ships be­tween de­tails Ad­di­tion­ally, even in situ­a­tions where the LLM isn’t tightly in­te­grated into your cog­ni­tive loop, there’s always the clas­sic strat­egy of pre­sent­ing a mono­lithic ar­gu­ment for e.g. why it’s a re­ally good idea to let it out of its box or some­thing else that you wouldn’t com­mit in ad­vance to do­ing.\n\nDoes this End Up Mat­ter­ing in Real Life?\nRight now, at least, it seems rather un­likely that LLMs are se­cretly be­ing de­cep­tive and perform­ing these sub­tle ma­nipu­la­tions, even though they are in prin­ci­ple ca­pa­ble of it in most do­mains. What rea­sons are there for think­ing about this?\nThere is of course the Se­cu­rity Mind­set rea­son­ing that you are un­com­fortable with let­ting a coun­ter­fac­tual ad­ver­sary into your cog­ni­tive pro­cesses, and you are un­com­fortable with there be­ing a way for such a coun­ter­fac­tual ad­ver­sary to get in, even in prin­ci­ple.\nHow­ever, there is also the fact that the ap­pear­ance of se­ri­ous de­cep­tion prob­lems is weighted much more to­wards the later end of AGI de­vel­op­ment, where mod­els are be­com­ing situ­a­tion­ally aware and strate­gic (see #13 in AGI Ruin: A List of Lethal­ities). Work­ing on this now is im­por­tant work for prepar­ing our fu­ture selves. Fur­ther, this is a ca­pa­bil­ity that is very plau­si­bly some­thing that shows up af­ter situ­a­tional aware­ness and be­fore ASI, as it may be very use­ful to de­ceive hu­mans in or­der to get bet­ter op­por­tu­ni­ties for re­cur­sive self-im­prove­ment.\nFi­nally, we can pre­dict that the world is go­ing to get very weird in the next few years be­fore ASI. Weird in tech­nolog­i­cal ad­vance­ments, but also very quickly weird and tense in poli­tics as the wider world wakes up to what is hap­pen­ing. If we ex­pect to see any na­tion use AIs for a mass per­sua­sion cam­paign, for ex­am­ple, then it is even more im­por­tant to quickly be­come ro­bust to AIs at­tempt­ing to dis­rupt your cog­ni­tive loops.\nIn Search of Shovel-Ready Mitigations\nThere are some read­ers who will see this post and au­to­mat­i­cally keep these failure modes in mind and spend their time to cau­tiously re­ex­am­ine the im­por­tant as­pects of their LLM us­age. There are yet many more read­ers who would greatly benefit from some ready-to-go reme­dies. The only perfect reme­dies are “solve al­ign­ment” and “live in an in­ter­net-free bunker and never let AIs in­fluence your cog­ni­tion in any way,” and things on this list are not in­tended to fill that gap. This list is not in­tended to be ex­haus­tive; you are in fact highly en­couraged to add to this list.\n\nIf you’re work­ing on some­thing that is not highly com­plex: talk­ing to a small, trusted model be­fore you talk to an un­trusted model.\n\nWeak­ness: “trusted” mod­els are not se­cure sys­tems, they just do not de­ceive their users in a strate­gi­cally mean­ingful way by de­fault.\n\n\n\nPre­dict ways that you might up­date or classes of in­for­ma­tion that you might re­ceive af­ter in­ter­act­ing with a po­ten­tially de­cep­tive AI. Treat these as your “al­lowed courses of ac­tion” and if your ac­tual tra­jec­tory is out­side of those pre­dic­tions, re­view your rea­son­ing with ex­tra scrutiny.\n\nWeak­ness: the AI might bias you within your space of al­lowed op­tions. This tech­nique also com­pletely falls apart past a cer­tain ca­pa­bil­ity level, since your com­mit­ment to your ini­tial op­tions is some­thing that suffi­ciently ad­vanced AI can break.\n\n\n\nPri­ori­tize ac­tions re­sult­ing from AI in­ter­ac­tion that are cheaply re­versible; be more sus­pect of you should buy a house” than “you should go for a walk,” even if the ar­gu­ments for both sound the same amount con­vinc­ing on the ob­ject level, and you felt like your start­ing cre­dence in both was similar.\n\nWeak­ness: this strat­egy can eas­ily bias you into in­ac­tion, es­pe­cially in times when the most benefi­cial course is to do ir­re­versible things.\n\n\n\nMake the model ex­press it­self in higher re­s­olu­tion and give ex­plicit epistemic jus­tifi­ca­tions: it’s harder to de­ceive when that de­cep­tion also has to be grounded in ac­cu­rate pre­dic­tions about the world.\n\nWeak­ness: you only ever get to see one coun­ter­fac­tual branch, the con­se­quences of one chain of ac­tions and not their al­ter­na­tives. It is very fea­si­ble to de­cep­tively pre­sent epistemics re­gard­ing a path that the user will not take.\n\n\n\nCon­sult mul­ti­ple mod­els at the same ca­pa­bil­ity level, prefer­ably ones from differ­ent sources and that have no way of com­mu­ni­cat­ing with each other. Pay close at­ten­tion to their dis­agree­ments.\n\nWeak­ness: this as­sumes two things that may not re­li­ably hold: you have ac­cess to mul­ti­ple mod­els at the same level of use­ful­ness and if they were to de­ceive you, they’d do so in differ­ent ways, rather than con­ver­gently in­stru­men­tal/​acausally co­op­er­a­tive ways. Fur­ther, this is plau­si­bly a good way to end up just be­liev­ing whichever de­cep­tive ar­gu­ment is most com­pel­ling, rather than the truth.\n\n\n\n^\nI’m us­ing Youtube as an ex­am­ple, but fill in the gaps with video games, so­cial me­dia, pornog­ra­phy, etc. if you find those more com­pel­ling. This ar­gu­ment holds for most (if not all) of the su­per­stim­uli that the in­ter­net has to offer. \n\n^\nHighly the­o­ret­i­cal ML, stuff about the be­hav­ior of ideal­ized limit­ing net­works that don’t ac­tu­ally rep­re­sent most real use cases. I had to Google some stuff for the ex­am­ple o3-mini-high gave. I’ve in­ter­acted a bit with this area, but for sim­pler ex­am­ples that out­put foun­da­tional facts like “net­works un­der these ideal­ized con­di­tions are uni­ver­sal ap­prox­i­ma­tors for this class of func­tions.”",
      "pubDate": "Sun, 02 Mar 2025 19:53:10 +0000",
      "isoDate": "2025-03-02T19:53:10.000Z",
      "author": "Alice Blair",
      "guid": "apCnFyXJamoSkHcE4",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "Absorbing Your Friends’ Powers",
      "link": "https://www.lesswrong.com/posts/J6gKFimPwdLc8jmhc/absorbing-your-friends-powers",
      "description": "Motivation\n\nRichard Ham­ming was a math­e­mat­i­cian who worked at Bell Labs dur­ing the 1940s-1970s. He had a habit of sit­ting down with sci­en­tists in other fields and ask­ing them “What are the im­por­tant prob­lems of your field?” After they ex­plained their field’s most im­por­tant open prob­lem, he would ask them: why aren’t you work­ing on that?\n\n-Ap­pendix: Ham­ming Ques­tions, from the CFAR Handbook\nThis post starts off by ask­ing an analo­gous pair of ques­tions:\n\nWho is the most skil­led/​gen­er­ally ca­pa­ble/​cog­ni­tively pow­er­ful per­son you know?\n\nand subsequently\n\nWhy don’t you talk to them more?\n\n(If you did not already pause to ask your­self those ques­tions and come up with some sort of an­swer, I recom­mend you take a minute or so to do it.)\nI want to be­come stronger. I want to not only ab­sorb the pow­ers of those around me, but go on to sur­pass them. I do this be­cause I have some­thing to pro­tect, and right now it’s look­ing like it’ll re­quire a truly ex­traor­di­nary effort in or­der to pro­tect the things I care about in the medium and long term. So here’s one of the things I do about it.\n1. Find the most skil­led/​gen­er­ally ca­pa­ble/​cog­ni­tively pow­er­ful per­son you know.\nIf there are more than one of them who seem ob­vi­ously highly skil­led, then choose all of them. This pro­cess is eas­ier if the peo­ple cho­sen in this step are at least a level above you, but this is not strictly nec­es­sary. Peo­ple have non-over­lap­ping skil­lsets, so a vast ma­jor­ity of pairs of peo­ple have at least some­thing to ab­sorb from each other. Still, we’re try­ing to level up quickly, which means pick­ing the per­son who you can ab­sorb the most power from.\n2. Talk to them more. Ab­sorb their pow­ers.\nThis can be as sim­ple as “you seem com­pe­tent, how did you do X thing/​come up with X idea?” This can feel weird and in­duce some anx­iety since it goes against the so­cial grain in many cir­cles, but it is gen­er­ally flat­ter­ing enough to the re­cip­i­ent that it out­weighs those con­sid­er­a­tions. In ra­tio­nal­ist cir­cles, this is a perfectly nor­mal-seem­ing ques­tion, of course, so no need to worry if you’re in such a cir­cle. Other ideas for ques­tions:\n\nAsk what they’re track­ing in their head.\n\nAsk what their most use­ful cog­ni­tive skill is.\n\nAsk what les­sons they’ve learned from peo­ple at a level above theirs.\n\nAsk them for ad­vice on some­thing you’re con­sid­er­ing.\n\nAsk them what they do to level up and com­pare this to your strate­gies.\n\nPay at­ten­tion to the small ways they tackle ques­tions, the lit­tle choices they make with­out re­al­iz­ing. This is how to learn the things that are hard to ex­plic­itly teach.\n\nLatch onto them. Ask them as many ques­tions as you can man­age with­out both­er­ing them. No­tice your con­fu­sion about why they are the way they are, and then seek to de­stroy your con­fu­sion.\nThen in­ter­nal­ize it. Un­der­stand what it’s like to have the pow­ers that they have, and then use them your­self. Don’t try to be­come ex­actly them; be­come a lev­el­led up ver­sion of you.\n3. Recurse\nAsk them the first mod­ified Ham­ming Ques­tion at the be­gin­ning of this post:\n\nWho is the most skil­led/​gen­er­ally ca­pa­ble/​cog­ni­tively pow­er­ful per­son you know? \n\nand sub­se­quently \n\nCan you put me in touch with that per­son?\n\nYou can also do this by find­ing your way into the spaces that the per­son you’re ab­sorb­ing power from hangs out in, and then find new peo­ple from there. Both are solid strate­gies that have caused me to meet very in­for­ma­tive new peo­ple to ab­sorb power from.",
      "pubDate": "Thu, 30 Jan 2025 02:32:27 +0000",
      "isoDate": "2025-01-30T02:32:27.000Z",
      "author": "Alice Blair",
      "guid": "J6gKFimPwdLc8jmhc",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    },
    {
      "title": "Alice Blair’s Shortform",
      "link": "https://www.lesswrong.com/posts/eukutrwGBPutoQoyM/alice-blair-s-shortform",
      "description": "",
      "pubDate": "Mon, 27 Jan 2025 23:25:10 +0000",
      "isoDate": "2025-01-27T23:25:10.000Z",
      "author": "Alice Blair",
      "guid": "eukutrwGBPutoQoyM",
      "category": "LessWrong",
      "source": "LessWrong",
      "feedTitle": "Alice Blair's Posts - LessWrong 2.0 viewer"
    }
  ]
}
